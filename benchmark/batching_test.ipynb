{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, PreTrainedTokenizerBase\n",
    "from langport.protocol.worker_protocol import (\n",
    "    GenerationTask,\n",
    ")\n",
    "from langport.model.executor.generation.huggingface import (\n",
    "    BaseStreamer,\n",
    "    BatchingTask,\n",
    "    GenerationModel\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mock_task(q:str='请写一首诗歌赞扬一下熬夜'):\n",
    "    prompt = f\"\"\"Consider a conversation between User (a human) and Assistant (named Buddy).                                                                                          \n",
    "Buddy is an INTP-T, a friendly, intelligent and multilingual AI assistant, by OpenBuddy team. GitHub: https://github.com/OpenBuddy/OpenBuddy                         \n",
    "Buddy cannot access the Internet.                                                                                                                                    \n",
    "Buddy can fluently speak the user's language (e.g. Chinese, English).                                                                                                \n",
    "Buddy can generate poems, stories, code, essays, songs, parodies, and more.                                                                                          \n",
    "Buddy possesses vast knowledge about the world, history, and culture.                                                                                                \n",
    "Buddy's responses are always safe, creative, high-quality, human-like, and interesting.                                                                              \n",
    "Buddy strictly refuses to discuss political, NSFW, or other unsafe topics.                                                                           \n",
    "                                                                                                                                                                     \n",
    "User: Hi.                                                                                                                                                            \n",
    "Assistant: Hi，我是 Buddy，你的 AI 助理。有什么我可以帮助你的么？                                                                                               \n",
    "User: {q}                                                                                                                                          \n",
    "Assistant:\"\"\"\n",
    "    return GenerationTask(prompt=prompt,\n",
    "                          max_tokens=2048,\n",
    "                          top_k=50,\n",
    "                          top_p=0.5,\n",
    "                          temperature=0.5,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchStreamer(BaseStreamer):\n",
    "    def __init__(self, batch_task: BatchingTask,tokenizer: PreTrainedTokenizerBase) -> None:\n",
    "        self.tokenizer = tokenizer\n",
    "        self.batch_task = batch_task\n",
    "\n",
    "    def put(self, value):\n",
    "        \"\"\"Function that is called by `.generate()` to push new tokens\"\"\"\n",
    "        for i in range(self.batch_task.batch_size):\n",
    "            text = self.tokenizer.decode(self.batch_task.get_generated_ids(i), skip_special_tokens=True)\n",
    "            print(f\"batch {i}: \", text)\n",
    "\n",
    "    def end(self):\n",
    "        \"\"\"Function that is called by `.generate()` to signal the end of generation\"\"\"\n",
    "        print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '../openbuddy-7b-v1.5-fp16/'\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, \n",
    "    device_map=\"auto\", \n",
    "    trust_remote_code=True, \n",
    "    torch_dtype=torch.float16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmodel = GenerationModel(model)\n",
    "tasks = [mock_task(\"请写一首诗歌赞扬一下熬夜\"), mock_task(\"今天天气怎么样？\")]\n",
    "inputs = BatchingTask(tasks, tokenizer, model.device.type)\n",
    "streamer = BatchStreamer(inputs, tokenizer)\n",
    "gmodel.generate(inputs, 40, streamer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
